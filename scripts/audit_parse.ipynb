{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53efa7e0",
   "metadata": {},
   "source": [
    "### Parse MNREGA Social Audit Zipped HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692025a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf7b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_html(file_path):\n",
    "    \"\"\"Extract all relevant information from a gzipped HTML file,\n",
    "       flattening the qualitative report tables into individual columns.\n",
    "    \"\"\"\n",
    "    # Read the HTML content from a gzipped file or a normal file.\n",
    "    if file_path.name.endswith('.html.gz'):\n",
    "        with gzip.open(file_path, 'rb') as f:\n",
    "            html_bytes = f.read()\n",
    "        html_content = html_bytes.decode('utf-8', errors='replace')\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    data = {}\n",
    "    \n",
    "    # -- Basic Information Fields --\n",
    "    basic_fields = {\n",
    "        'state': 'ctl00_ContentPlaceHolder1_lblstate',\n",
    "        'district': 'ctl00_ContentPlaceHolder1_lbldistrict',\n",
    "        'block': 'ctl00_ContentPlaceHolder1_lblblock',\n",
    "        'panchayat': 'ctl00_ContentPlaceHolder1_lblpanchayat',\n",
    "        'sa_start_date': 'ctl00_ContentPlaceHolder1_lblSA_start_dt',\n",
    "        'sa_end_date': 'ctl00_ContentPlaceHolder1_lblSA_end_dt',\n",
    "        'gram_sabha_date': 'ctl00_ContentPlaceHolder1_lblGramSabha_dt',\n",
    "        'public_hearing_date': 'ctl00_ContentPlaceHolder1_lblPublic_Hearing_dt'\n",
    "    }\n",
    "    for key, id_value in basic_fields.items():\n",
    "        element = soup.find('span', id=id_value)\n",
    "        data[key] = element.text.strip() if element else None\n",
    "\n",
    "    # -- Financial Information Fields --\n",
    "    financial_fields = {\n",
    "        'sa_period_from': 'ctl00_ContentPlaceHolder1_lblSA_Period_From_Date',\n",
    "        'sa_period_to': 'ctl00_ContentPlaceHolder1_lblSA_Period_To_Date',\n",
    "        'wage_exp': 'ctl00_ContentPlaceHolder1_lblWage_exp',\n",
    "        'material_exp': 'ctl00_ContentPlaceHolder1_lblmat_exp',\n",
    "        'total_exp': 'ctl00_ContentPlaceHolder1_lbltotal_expen',\n",
    "        'wage_given': 'ctl00_ContentPlaceHolder1_lblwage_given',\n",
    "        'material_given': 'ctl00_ContentPlaceHolder1_lblmat_given',\n",
    "        'total_given': 'ctl00_ContentPlaceHolder1_lbltotal_record_given'\n",
    "    }\n",
    "    for key, id_value in financial_fields.items():\n",
    "        element = soup.find('span', id=id_value)\n",
    "        data[key] = element.text.strip() if element else None\n",
    "\n",
    "    # -- Work Information Fields --\n",
    "    work_fields = {\n",
    "        'total_works': 'ctl00_ContentPlaceHolder1_lbltot_work',\n",
    "        'total_households': 'ctl00_ContentPlaceHolder1_lbltot_hh',\n",
    "        'works_verified': 'ctl00_ContentPlaceHolder1_lbltot_work_verified',\n",
    "        'households_verified': 'ctl00_ContentPlaceHolder1_lbltot_hh_verified',\n",
    "        'gram_sabha_participants': 'ctl00_ContentPlaceHolder1_lblno_of_ppl_participated_gs'\n",
    "    }\n",
    "    for key, id_value in work_fields.items():\n",
    "        element = soup.find('span', id=id_value)\n",
    "        data[key] = element.text.strip() if element else None\n",
    "\n",
    "    # -- Expense Information Fields --\n",
    "    expense_fields = {\n",
    "        'printing_expense': 'ctl00_ContentPlaceHolder1_lblprinting_expense',\n",
    "        'videography_expense': 'ctl00_ContentPlaceHolder1_lblvideography_expense',\n",
    "        'tea_expense': 'ctl00_ContentPlaceHolder1_lbltea_expense',\n",
    "        'vrp_training_expense': 'ctl00_ContentPlaceHolder1_lblvrp_training_expense',\n",
    "        'vrp_travel_expense': 'ctl00_ContentPlaceHolder1_lblvrp_travel_expense',\n",
    "        'photocopying_expense': 'ctl00_ContentPlaceHolder1_lblphotocopying_expense',\n",
    "        'other_expense': 'ctl00_ContentPlaceHolder1_lblother_expense',\n",
    "        'vrp_honorarium_expense': 'ctl00_ContentPlaceHolder1_lblvrp_honorium_expense',\n",
    "        'stationary_expense': 'ctl00_ContentPlaceHolder1_lblstationary_expense',\n",
    "        'publicity_expense': 'ctl00_ContentPlaceHolder1_lblpublicity_expense',\n",
    "        'mic_expense': 'ctl00_ContentPlaceHolder1_lblmic_expense',\n",
    "        'photography_expense': 'ctl00_ContentPlaceHolder1_lblphotography_expense',\n",
    "        'shamiana_expense': 'ctl00_ContentPlaceHolder1_lblshamiana_expense',\n",
    "        'total_expense': 'ctl00_ContentPlaceHolder1_lbltotal_expense'\n",
    "    }\n",
    "    for key, id_value in expense_fields.items():\n",
    "        element = soup.find('span', id=id_value)\n",
    "        data[key] = element.text.strip() if element else None\n",
    "\n",
    "    # -- Checklist Information Fields --\n",
    "    checklist_fields = {\n",
    "        'job_cards_with_people': 'ctl00_ContentPlaceHolder1_Label1',\n",
    "        'job_cards_updated': 'ctl00_ContentPlaceHolder1_Label3',\n",
    "        'job_cards_renewed': 'ctl00_ContentPlaceHolder1_Label4',\n",
    "        'demand_registration_process': 'ctl00_ContentPlaceHolder1_Label2',\n",
    "        'unmet_demand': 'ctl00_ContentPlaceHolder1_Label29',\n",
    "        'payment_agency_problems': 'ctl00_ContentPlaceHolder1_Label30'\n",
    "    }\n",
    "    for key, id_value in checklist_fields.items():\n",
    "        element = soup.find('span', id=id_value)\n",
    "        data[key] = element.text.strip() if element else None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Qualitative Report: Summary Of Reported Issues Table\n",
    "    # ---------------------------\n",
    "    rep_div = soup.find(\"div\", id=\"ctl00_ContentPlaceHolder1_divReportedIssue\")\n",
    "    if rep_div:\n",
    "        rep_table = rep_div.find(\"table\")\n",
    "        if rep_table:\n",
    "            # Get the two header rows:\n",
    "            thead = rep_table.find(\"thead\")\n",
    "            if thead:\n",
    "                header_rows = thead.find_all(\"tr\")\n",
    "            else:\n",
    "                header_rows = rep_table.find_all(\"tr\")[:2]\n",
    "            if len(header_rows) >= 2:\n",
    "                # Process top header row (expand by colspan)\n",
    "                top_row = header_rows[0]\n",
    "                top_headers = []\n",
    "                for th in top_row.find_all(\"th\"):\n",
    "                    colspan = int(th.get(\"colspan\", 1))\n",
    "                    text = th.get_text(strip=True)\n",
    "                    top_headers.extend([text] * colspan)\n",
    "                # Process bottom header row\n",
    "                bottom_row = header_rows[1]\n",
    "                bottom_headers = [th.get_text(strip=True) for th in bottom_row.find_all(\"th\")]\n",
    "                # Combine the two layers, skipping the first column (SR#)\n",
    "                combined_headers = []\n",
    "                # Assume the first cell in bottom_headers is \"SR#\" and skip it.\n",
    "                for i in range(1, len(bottom_headers)):\n",
    "                    group = top_headers[i]\n",
    "                    sub = bottom_headers[i]\n",
    "                    # Combine group and subheader if group is not already contained in sub.\n",
    "                    if group and (group.lower() not in sub.lower()):\n",
    "                        combined = f\"{group}_{sub}\"\n",
    "                    else:\n",
    "                        combined = sub\n",
    "                    combined = combined.lower().replace(\" \", \"_\")\n",
    "                    combined_headers.append(combined)\n",
    "                # Extract the first data row (skip the first cell)\n",
    "                tbody = rep_table.find(\"tbody\")\n",
    "                if tbody:\n",
    "                    data_row = tbody.find(\"tr\")\n",
    "                else:\n",
    "                    all_rows = rep_table.find_all(\"tr\")\n",
    "                    data_row = all_rows[2] if len(all_rows) > 2 else None\n",
    "                if data_row:\n",
    "                    values = [td.get_text(strip=True) for td in data_row.find_all(\"td\")]\n",
    "                    # Skip the first value (SR#)\n",
    "                    values = values[1:]\n",
    "                else:\n",
    "                    values = []\n",
    "                # If the headers and values match, add them as separate keys.\n",
    "                if len(combined_headers) == len(values):\n",
    "                    for header, value in zip(combined_headers, values):\n",
    "                        key = f\"rep_{header}\"\n",
    "                        data[key] = value\n",
    "                else:\n",
    "                    data[\"rep_reported_issues_table\"] = rep_table.get_text(separator=\" | \", strip=True)\n",
    "            else:\n",
    "                data[\"rep_reported_issues\"] = rep_table.get_text(separator=\" | \", strip=True)\n",
    "        else:\n",
    "            data[\"rep_reported_issues\"] = rep_div.get_text(separator=\" | \", strip=True)\n",
    "    else:\n",
    "        data[\"rep_reported_issues\"] = None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Qualitative Report: Summary Of Action Taken Report Table\n",
    "    # ---------------------------\n",
    "    atr_div = soup.find(\"div\", id=\"ctl00_ContentPlaceHolder1_divATR\")\n",
    "    if atr_div:\n",
    "        atr_table = atr_div.find(\"table\")\n",
    "        if atr_table:\n",
    "            header_row = atr_table.find(\"tr\")\n",
    "            if header_row:\n",
    "                headers = [th.get_text(strip=True) for th in header_row.find_all([\"th\", \"td\"])]\n",
    "                # Remove the first header if it is \"SR#\" or similar.\n",
    "                if headers and headers[0].strip().lower() in [\"sr#\", \"sr\"]:\n",
    "                    headers = headers[1:]\n",
    "            else:\n",
    "                headers = []\n",
    "            tbody = atr_table.find(\"tbody\")\n",
    "            if tbody:\n",
    "                data_row = tbody.find(\"tr\")\n",
    "            else:\n",
    "                all_rows = atr_table.find_all(\"tr\")\n",
    "                data_row = all_rows[1] if len(all_rows) >= 2 else None\n",
    "            if data_row:\n",
    "                values = [td.get_text(strip=True) for td in data_row.find_all(\"td\")]\n",
    "                # Remove the first value if it corresponds to SR#\n",
    "                if len(values) == len(headers) + 1:\n",
    "                    values = values[1:]\n",
    "            else:\n",
    "                values = []\n",
    "            if len(headers) == len(values):\n",
    "                for header, value in zip(headers, values):\n",
    "                    key = f\"atr_{header.lower().replace(' ', '_')}\"\n",
    "                    data[key] = value\n",
    "            else:\n",
    "                data[\"atr_report\"] = atr_table.get_text(separator=\" | \", strip=True)\n",
    "        else:\n",
    "            data[\"atr_report\"] = atr_div.get_text(separator=\" | \", strip=True)\n",
    "    else:\n",
    "        data[\"atr_report\"] = None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Gram Panchayat Checklist Section\n",
    "    # ---------------------------\n",
    "    checklist_heading = soup.find(\"div\", class_=\"panel-heading\", string=lambda t: t and \"Gram Panchayat Checklist\" in t)\n",
    "    gram_checklist = {}\n",
    "    if checklist_heading:\n",
    "        checklist_container = checklist_heading.find_next_sibling(\"div\", class_=\"panel-body\")\n",
    "        if checklist_container:\n",
    "            inner_panels = checklist_container.find_all(\"div\", class_=\"panel panel-info\")\n",
    "            for panel in inner_panels:\n",
    "                section_heading_elem = panel.find(\"div\", class_=\"panel-heading\")\n",
    "                section_name = section_heading_elem.get_text(strip=True) if section_heading_elem else \"unknown_section\"\n",
    "                panel_body = panel.find(\"div\", class_=\"panel-body\")\n",
    "                section_data = {}\n",
    "                if panel_body:\n",
    "                    ul = panel_body.find(\"ul\", class_=\"man\")\n",
    "                    if ul:\n",
    "                        li_items = ul.find_all(\"li\")\n",
    "                        for li in li_items:\n",
    "                            full_text = li.get_text(\" \", strip=True)\n",
    "                            parts = full_text.split(\":\", 1)\n",
    "                            if len(parts) == 2:\n",
    "                                question = parts[0].strip().lower().replace(\" \", \"_\")\n",
    "                                answer = parts[1].strip()\n",
    "                            else:\n",
    "                                question = full_text.lower().replace(\" \", \"_\")\n",
    "                                answer = \"\"\n",
    "                            section_data[question] = answer\n",
    "                gram_checklist[section_name.lower().replace(\" \", \"_\")] = section_data\n",
    "    data[\"gram_panchayat_checklist\"] = gram_checklist if gram_checklist else None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Reported Issues Section (if needed separately)\n",
    "    # ---------------------------\n",
    "    issues_div = soup.find('div', id=\"ctl00_ContentPlaceHolder1_divReportedIssue\")\n",
    "    if issues_div:\n",
    "        data['reported_issues'] = issues_div.get_text(separator=\" \", strip=True)\n",
    "    else:\n",
    "        data['reported_issues'] = None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Source File Reference\n",
    "    # ---------------------------\n",
    "    data['source_file'] = file_path.name\n",
    "\n",
    "    return data\n",
    "\n",
    "def delete_invalid_gz_files(folder_paths):\n",
    "    \"\"\"\n",
    "    Scans multiple folders for invalid .gz files and deletes them.\n",
    "    \n",
    "    Args:\n",
    "        folder_paths (list): A list of directory paths to scan.\n",
    "    \"\"\"\n",
    "    for folder_path in folder_paths:\n",
    "        folder = Path(folder_path)\n",
    "\n",
    "        if not folder.exists():\n",
    "            logging.warning(f\"⚠️ Warning: Folder {folder} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        gz_files = list(folder.glob('*.gz'))\n",
    "\n",
    "        logging.info(f\"📂 Checking {len(gz_files)} .gz files in {folder}...\")\n",
    "\n",
    "        for file_path in gz_files:\n",
    "            try:\n",
    "                # Try to open and read a small portion of the file\n",
    "                with gzip.open(file_path, 'rb') as f:\n",
    "                    f.read(1)  # Read a small part to check validity\n",
    "            except (OSError, gzip.BadGzipFile):\n",
    "                logging.warning(f\"🗑️ Deleting invalid .gz file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "\n",
    "    logging.info(\"✅ Finished scanning and deleting invalid .gz files.\")\n",
    "\n",
    "def process_html_folders(folder_paths, max_workers=8):\n",
    "    \"\"\"\n",
    "    Process all gzipped HTML files (.html.gz) in the given list of folders concurrently\n",
    "    and return a combined DataFrame.\n",
    "    \n",
    "    :param folder_paths: List of directories to process.\n",
    "    :param max_workers: Number of worker threads to use.\n",
    "    :return: Combined DataFrame with extracted HTML data.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    futures = []\n",
    "    \n",
    "    # Create a thread pool executor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for folder_path in folder_paths:\n",
    "            folder = Path(folder_path)\n",
    "            html_files = list(folder.glob('*.html.gz'))\n",
    "            print(f\"📂 Found {len(html_files)} .html.gz files in {folder}...\")\n",
    "            \n",
    "            for file_path in html_files:\n",
    "                futures.append(executor.submit(extract_info_from_html, file_path))\n",
    "        \n",
    "        # Iterate through the completed futures (using tqdm for progress)\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "            try:\n",
    "                data = future.result()\n",
    "                all_results.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error processing file: {e}\")\n",
    "\n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        output_file = '../data/final_audit_results.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n✅ Saved final results to {output_file}\")\n",
    "        print(f\"📊 Total records processed: {len(df)}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e34372-7674-41cf-93ef-d3fd780c5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_folders = [\"../data/html\"]\n",
    "delete_invalid_gz_files(html_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f58c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 296656 .html.gz files in ../data/html...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  99%|██████████████████████████████████████████████▍| 293051/296656 [5:39:25<1:39:16,  1.65s/it]IOStream.flush timed out\n",
      "Processing files: 100%|████████████████████████████████████████████████▊| 295612/296656 [5:43:52<36:24,  2.09s/it]IOStream.flush timed out\n",
      "Processing files: 100%|████████████████████████████████████████████████▉| 296034/296656 [5:44:34<00:41, 15.08it/s]IOStream.flush timed out\n",
      "Processing files: 100%|█████████████████████████████████████████████████| 296656/296656 [5:45:43<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved final results to ../data/final_audit_results.csv\n",
      "📊 Total records processed: 296656\n"
     ]
    }
   ],
   "source": [
    "df = process_html_folders(html_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204e76ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>block</th>\n",
       "      <th>panchayat</th>\n",
       "      <th>sa_start_date</th>\n",
       "      <th>sa_end_date</th>\n",
       "      <th>gram_sabha_date</th>\n",
       "      <th>public_hearing_date</th>\n",
       "      <th>sa_period_from</th>\n",
       "      <th>sa_period_to</th>\n",
       "      <th>...</th>\n",
       "      <th>atr_fd_amount</th>\n",
       "      <th>atr_amount_of_fine/penalty_paid</th>\n",
       "      <th>atr_number_of_firs_filled</th>\n",
       "      <th>atr_number_of_employees_suspended</th>\n",
       "      <th>atr_number_of_employees_terminated</th>\n",
       "      <th>gram_panchayat_checklist</th>\n",
       "      <th>reported_issues</th>\n",
       "      <th>source_file</th>\n",
       "      <th>rep_reported_issues</th>\n",
       "      <th>atr_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAJASTHAN</td>\n",
       "      <td>NAGAUR</td>\n",
       "      <td>DEGANA</td>\n",
       "      <td>आंतरोली कलां</td>\n",
       "      <td>19/06/2024</td>\n",
       "      <td>23/06/2024</td>\n",
       "      <td>24/06/2024</td>\n",
       "      <td>24/06/2024</td>\n",
       "      <td>01/04/2023</td>\n",
       "      <td>31/03/2024</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'job_cards': {'are_job_cards_with_people': 'B...</td>\n",
       "      <td>Summary Of Reported Issues Financial Misapprop...</td>\n",
       "      <td>27_2714_2714007_2714007275_2023-2024_6_24_2024...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEGHALAYA</td>\n",
       "      <td>EAST KHASI HILLS</td>\n",
       "      <td>MAWPHLANG</td>\n",
       "      <td>Lawkhla Mawlong</td>\n",
       "      <td>31/07/2023</td>\n",
       "      <td>03/08/2023</td>\n",
       "      <td>04/08/2023</td>\n",
       "      <td>29/02/2024</td>\n",
       "      <td>01/10/2022</td>\n",
       "      <td>31/03/2023</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'job_cards': {'are_job_cards_with_people': 'B...</td>\n",
       "      <td>Summary Of Reported Issues Financial Misapprop...</td>\n",
       "      <td>21_2102_2102005_2102009052_2022-2023_8_4_2023_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEGHALAYA</td>\n",
       "      <td>EAST KHASI HILLS</td>\n",
       "      <td>KHADARSHNONG-LAITKROH</td>\n",
       "      <td>Myiong</td>\n",
       "      <td>26/06/2023</td>\n",
       "      <td>28/06/2023</td>\n",
       "      <td>28/06/2023</td>\n",
       "      <td>26/07/2023</td>\n",
       "      <td>01/10/2022</td>\n",
       "      <td>31/03/2023</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'job_cards': {'are_job_cards_with_people': 'G...</td>\n",
       "      <td>Summary Of Reported Issues Financial Misapprop...</td>\n",
       "      <td>21_2102_2102002_2102002026_2022-2023_6_28_2023...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UTTAR PRADESH</td>\n",
       "      <td>KANNAUJ</td>\n",
       "      <td>TALGRAM</td>\n",
       "      <td>Terajaket</td>\n",
       "      <td>22/07/2019</td>\n",
       "      <td>24/07/2019</td>\n",
       "      <td>24/07/2019</td>\n",
       "      <td>01/08/2019</td>\n",
       "      <td>01/04/2018</td>\n",
       "      <td>31/03/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'job_cards': {'are_job_cards_with_people': 'G...</td>\n",
       "      <td>Summary Of Reported Issues Financial Misapprop...</td>\n",
       "      <td>31_3168_3168006_3168006046_2018-2019_7_24_2019...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BIHAR</td>\n",
       "      <td>AURANAGABAD</td>\n",
       "      <td>GOH</td>\n",
       "      <td>MIRPUR</td>\n",
       "      <td>02/11/2022</td>\n",
       "      <td>06/11/2022</td>\n",
       "      <td>07/11/2022</td>\n",
       "      <td>07/11/2022</td>\n",
       "      <td>01/04/2021</td>\n",
       "      <td>31/03/2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'job_cards': {'are_job_cards_with_people': 'G...</td>\n",
       "      <td>Summary Of Reported Issues Financial Misapprop...</td>\n",
       "      <td>05_0505_0505004_0505004006_2021-2022_11_7_2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           state          district                  block        panchayat  \\\n",
       "0      RAJASTHAN            NAGAUR                 DEGANA     आंतरोली कलां   \n",
       "1      MEGHALAYA  EAST KHASI HILLS              MAWPHLANG  Lawkhla Mawlong   \n",
       "2      MEGHALAYA  EAST KHASI HILLS  KHADARSHNONG-LAITKROH           Myiong   \n",
       "3  UTTAR PRADESH           KANNAUJ                TALGRAM        Terajaket   \n",
       "4          BIHAR       AURANAGABAD                    GOH           MIRPUR   \n",
       "\n",
       "  sa_start_date sa_end_date gram_sabha_date public_hearing_date  \\\n",
       "0    19/06/2024  23/06/2024      24/06/2024          24/06/2024   \n",
       "1    31/07/2023  03/08/2023      04/08/2023          29/02/2024   \n",
       "2    26/06/2023  28/06/2023      28/06/2023          26/07/2023   \n",
       "3    22/07/2019  24/07/2019      24/07/2019          01/08/2019   \n",
       "4    02/11/2022  06/11/2022      07/11/2022          07/11/2022   \n",
       "\n",
       "  sa_period_from sa_period_to  ... atr_fd_amount  \\\n",
       "0     01/04/2023   31/03/2024  ...             0   \n",
       "1     01/10/2022   31/03/2023  ...             0   \n",
       "2     01/10/2022   31/03/2023  ...             0   \n",
       "3     01/04/2018   31/03/2019  ...             0   \n",
       "4     01/04/2021   31/03/2022  ...             0   \n",
       "\n",
       "  atr_amount_of_fine/penalty_paid atr_number_of_firs_filled  \\\n",
       "0                               0                         0   \n",
       "1                               0                         0   \n",
       "2                               0                         0   \n",
       "3                               0                         0   \n",
       "4                               0                         0   \n",
       "\n",
       "  atr_number_of_employees_suspended atr_number_of_employees_terminated  \\\n",
       "0                                 0                                  0   \n",
       "1                                 0                                  0   \n",
       "2                                 0                                  0   \n",
       "3                                 0                                  0   \n",
       "4                                 0                                  0   \n",
       "\n",
       "                            gram_panchayat_checklist  \\\n",
       "0  {'job_cards': {'are_job_cards_with_people': 'B...   \n",
       "1  {'job_cards': {'are_job_cards_with_people': 'B...   \n",
       "2  {'job_cards': {'are_job_cards_with_people': 'G...   \n",
       "3  {'job_cards': {'are_job_cards_with_people': 'G...   \n",
       "4  {'job_cards': {'are_job_cards_with_people': 'G...   \n",
       "\n",
       "                                     reported_issues  \\\n",
       "0  Summary Of Reported Issues Financial Misapprop...   \n",
       "1  Summary Of Reported Issues Financial Misapprop...   \n",
       "2  Summary Of Reported Issues Financial Misapprop...   \n",
       "3  Summary Of Reported Issues Financial Misapprop...   \n",
       "4  Summary Of Reported Issues Financial Misapprop...   \n",
       "\n",
       "                                         source_file rep_reported_issues  \\\n",
       "0  27_2714_2714007_2714007275_2023-2024_6_24_2024...                 NaN   \n",
       "1  21_2102_2102005_2102009052_2022-2023_8_4_2023_...                 NaN   \n",
       "2  21_2102_2102002_2102002026_2022-2023_6_28_2023...                 NaN   \n",
       "3  31_3168_3168006_3168006046_2018-2019_7_24_2019...                 NaN   \n",
       "4  05_0505_0505004_0505004006_2021-2022_11_7_2022...                 NaN   \n",
       "\n",
       "  atr_report  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd9c49b-3fa6-48ac-b179-c45c2f81e93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296656, 63)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9ae390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    \"\"\"\n",
    "    Recursively flattens a nested dictionary.\n",
    "    For example:\n",
    "      {'job_cards': {'are_job_cards_with_people': 'Greater than 75%', 'are_job_cards_updated': 'Yes'}}\n",
    "    becomes:\n",
    "      {'job_cards_are_job_cards_with_people': 'Greater than 75%', 'job_cards_are_job_cards_updated': 'Yes'}\n",
    "    \"\"\"\n",
    "    items = {}\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_dict(v, new_key, sep=sep))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_checklist_df = pd.json_normalize(df['gram_panchayat_checklist'])\n",
    "\n",
    "flat_checklist_df = flat_checklist_df.add_prefix(\"gpc_\")\n",
    "\n",
    "df = df.drop(columns=['gram_panchayat_checklist']).join(flat_checklist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4290155-2b00-4fab-8e5f-40e474a2cbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296656, 91)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f026068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/final_audit_results.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c33cb-1d75-4805-8501-3fc3ccbd9f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
